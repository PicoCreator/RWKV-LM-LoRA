{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfCtx trainer validation\n",
    "This model being trained has the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "The goal is to validate loss rate change, across the exact same hyper parameters\n",
    "- 1024 data chunk size\n",
    "- same learningrate / weightdecay / seed\n",
    "\n",
    "With only the change in working context size\n",
    "- 1024 context vs 128 context\n",
    "\n",
    "> This project assumes you have the rwkv-exp conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-26 01:59:02--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 18.154.227.87, 18.154.227.69, 18.154.227.67, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.154.227.87|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1688003942&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2VmL2NiZWYwOWFiYjI2MzRhMzM3NWIyODg2OGJmZmEyODUyMjZkZmVhYmVkZWM4OWIyOGMyZmIzMDIyMjExNjRkNjYvMGVjNzIxNGVkMTY3MzdhNjM0ODI1NGU2Zjk2ZDhjZGMwNGQzYjVlZmJkNWY1M2ZlOTMzNzYwN2VhNDJiNWI5Zj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgwMDM5NDJ9fX1dfQ__&Signature=PEwWJWvgnbSXIeiGDSuJN40RlktuCNR-O5UUR6%7EK64uuGOXPyBcZ7ebiPRVIcleXBXAoOyoGuInKbfK1iN5-wAKvKEV0hfUhVAJYLeoiIGF2pjiNS4-PU5%7E2uggc9DGFsHlMSy4swy6TH3ZRgzU8WlFBAl1Dy3mhjIEnDU8XLTFnnIGBcBACVYCKTj3uBiAl3Vy6PeQodZ4igI2sHEOKK9DyP4WuwKQs1zw5HcWKHgGTD9rNCFi8k5neTy50-3eHF2kMJLNPz%7EwbVUFdzd4A3WjZavPeOV4fr9TC0qpKg6YmLb9boIkvtV2nrR8fEQDWldkyW2yUdDtaFf4Igp6WjA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-06-26 01:59:02--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1688003942&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2VmL2NiZWYwOWFiYjI2MzRhMzM3NWIyODg2OGJmZmEyODUyMjZkZmVhYmVkZWM4OWIyOGMyZmIzMDIyMjExNjRkNjYvMGVjNzIxNGVkMTY3MzdhNjM0ODI1NGU2Zjk2ZDhjZGMwNGQzYjVlZmJkNWY1M2ZlOTMzNzYwN2VhNDJiNWI5Zj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgwMDM5NDJ9fX1dfQ__&Signature=PEwWJWvgnbSXIeiGDSuJN40RlktuCNR-O5UUR6%7EK64uuGOXPyBcZ7ebiPRVIcleXBXAoOyoGuInKbfK1iN5-wAKvKEV0hfUhVAJYLeoiIGF2pjiNS4-PU5%7E2uggc9DGFsHlMSy4swy6TH3ZRgzU8WlFBAl1Dy3mhjIEnDU8XLTFnnIGBcBACVYCKTj3uBiAl3Vy6PeQodZ4igI2sHEOKK9DyP4WuwKQs1zw5HcWKHgGTD9rNCFi8k5neTy50-3eHF2kMJLNPz%7EwbVUFdzd4A3WjZavPeOV4fr9TC0qpKg6YmLb9boIkvtV2nrR8fEQDWldkyW2yUdDtaFf4Igp6WjA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.111, 108.138.64.36, 108.138.64.121, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.111|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3030345861 (2.8G) [binary/octet-stream]\n",
      "Saving to: ‘Echo-A-1B5-Init.pth’\n",
      "\n",
      "Echo-A-1B5-Init.pth 100%[===================>]   2.82G  55.3MB/s    in 54s     \n",
      "\n",
      "2023-06-26 01:59:56 (53.5 MB/s) - ‘Echo-A-1B5-Init.pth’ saved [3030345861/3030345861]\n",
      "\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 2.9G Jun 22 04:41 ./model/Echo-A-1B5-Init.pth\n"
     ]
    }
   ],
   "source": [
    "# First lets get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "#\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ./model/\n",
    "!rm -rf ./model/Echo-A-1B5-Init.pth\n",
    "!cd ./model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
    "!ls -alh ./model/Echo-A-1B5-Init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets ensure some of the directories we need are setup\n",
    "!mkdir -p ./datapaths/\n",
    "!mkdir -p ./checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████| 424/424 [00:00<00:00, 3.46MB/s]\n",
      "Downloading and preparing dataset None/None to /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/15.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|█████████████████████| 15.2M/15.2M [00:00<00:00, 113MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2162.01it/s]\n",
      "Setting num_proc from 32 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset parquet downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1032.32it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ./RWKV-v4neo && python3 preload_dataset.py ./Mini-Enwiki-Test-A.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context Size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "usage: new_train.py [-h] [-c CONFIG] [--print_config[=flags]]\n",
      "                    {fit,validate,test,predict} ...\n",
      "new_train.py: error: 'Configuration check failed :: No action for destination key \"dataset_loader.test\" to check its value.'\n"
     ]
    }
   ],
   "source": [
    "# Lets start the training process\n",
    "!cd ./RWKV-v4neo && python new_train.py fit -c ./Mini-Enwiki-Test-A.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230626_121748-aenwikm32\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMini-Enwiki-Test-B (no deepspeed checkpoint, lr 3e-4/1e-5/5, wd 0.01, ctx 128, data 1024, fixed seed, bs 16)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/aenwikm32\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_128_bf16...\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 935.18it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-41918cf80015bf90_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f140cf11844ffa70.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-92fac1a21ac1344f_*_of_00032.arrow\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-06-26 12:18:04,047] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/ubuntu/RWKV-LM-LoRA/checkpoint/Mini-Enwiki-Test-B exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.338107109069824 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.061330556869506836 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006895065307617188 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/5308 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/RWKV-LM-LoRA/RWKV-v4neo/new_train.py\", line 14, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/ubuntu/RWKV-LM-LoRA/RWKV-v4neo/new_train.py\", line 11, in cli_main\n",
      "    LightningCLI(RWKV, get_data_module, save_config_kwargs={\"overwrite\": True})\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 133, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 218, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 185, in run\n",
      "    self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 261, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 142, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1265, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 158, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 259, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 224, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 92, in optimizer_step\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 135, in closure\n",
      "    self._backward_fn(step_output.closure_loss)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 233, in backward_fn\n",
      "    call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 199, in backward\n",
      "    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 81, in backward\n",
      "    deepspeed_engine.backward(tensor, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1862, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1901, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/autograd/function.py\", line 274, in apply\n",
      "    return user_fn(self, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 684, in backward\n",
      "    torch.autograd.backward(output_tensors, grad_tensors)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 810, in reduce_partition_and_remove_grads\n",
      "    self.reduce_ready_partitions_and_remove_grads(param, i)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1258, in reduce_ready_partitions_and_remove_grads\n",
      "    self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 843, in reduce_independent_p_g_buckets_and_remove_grads\n",
      "    assert self.params_already_reduced[param_id] == False, \\\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: The parameter 266 has already been reduced.             Gradient computed twice for this partition.             Multiple gradient reduction is currently not supported\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 10.93756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mMini-Enwiki-Test-B (no deepspeed checkpoint, lr 3e-4/1e-5/5, wd 0.01, ctx 128, data 1024, fixed seed, bs 16)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/aenwikm32\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230626_121748-aenwikm32/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lets start the training process\n",
    "!cd ./RWKV-v4neo && python new_train.py fit -c ./Mini-Enwiki-Test-B.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
