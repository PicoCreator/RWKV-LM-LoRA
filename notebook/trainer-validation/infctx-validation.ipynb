{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfCtx trainer validation\n",
    "This model being trained has the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "The goal is to validate loss rate change, across the exact same hyper parameters with the following\n",
    "- 1024 data chunk size\n",
    "- same learningrate / weightdecay / seed\n",
    "- \"teven/enwiki_10k\" dataset, chunked to 1024 token sizes\n",
    "\n",
    "With only the change in training context size\n",
    "- 1024 context vs 128 context\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    ">\n",
    "> All training runs (except dryrun) is configured to log to weights and bias, comment out the logger in the config file if you want to avoid this\n",
    ">\n",
    "> Due to existing \"hang\" issues with multi-gpu with bptt_length > 1, segmented training is limited to 1 gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the init model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-05 01:12:44--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.70, 99.84.108.129, 99.84.108.55, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1688778765&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2VmL2NiZWYwOWFiYjI2MzRhMzM3NWIyODg2OGJmZmEyODUyMjZkZmVhYmVkZWM4OWIyOGMyZmIzMDIyMjExNjRkNjYvMGVjNzIxNGVkMTY3MzdhNjM0ODI1NGU2Zjk2ZDhjZGMwNGQzYjVlZmJkNWY1M2ZlOTMzNzYwN2VhNDJiNWI5Zj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODg3Nzg3NjV9fX1dfQ__&Signature=HIB7GtShsj%7EFU30MzimrjBKfTZ98lRemyl1PME3E8TarRhIyxfRVt-Y1E8n8HevNvid%7EqwjYVJiVr4LK6V5kTDpL1UMk6va6-vI8WM4Fbq0T8MTxTV%7EQDUDJg5WUPciH%7Ek%7ErIlYNXni8guPyrtHD3tGYcdPr4C46YSdTEOyW4htApE7PzS12Qd5hZoPhN4fgyzCsLtPcuSLrM6mecW3F2frELJ3IQkz6bYcOIiEkmcmMq3l5pO-cHMFCiXLs1VhD9ii8m9qrV3tWUDpeC2r4s-ByRFxi8VStDFU9OddNyBwD94h9B4iKNWMc8qOjiltLm0o%7EPeYfFmjduEAaBO5IqA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-05 01:12:44--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1688778765&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2VmL2NiZWYwOWFiYjI2MzRhMzM3NWIyODg2OGJmZmEyODUyMjZkZmVhYmVkZWM4OWIyOGMyZmIzMDIyMjExNjRkNjYvMGVjNzIxNGVkMTY3MzdhNjM0ODI1NGU2Zjk2ZDhjZGMwNGQzYjVlZmJkNWY1M2ZlOTMzNzYwN2VhNDJiNWI5Zj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODg3Nzg3NjV9fX1dfQ__&Signature=HIB7GtShsj%7EFU30MzimrjBKfTZ98lRemyl1PME3E8TarRhIyxfRVt-Y1E8n8HevNvid%7EqwjYVJiVr4LK6V5kTDpL1UMk6va6-vI8WM4Fbq0T8MTxTV%7EQDUDJg5WUPciH%7Ek%7ErIlYNXni8guPyrtHD3tGYcdPr4C46YSdTEOyW4htApE7PzS12Qd5hZoPhN4fgyzCsLtPcuSLrM6mecW3F2frELJ3IQkz6bYcOIiEkmcmMq3l5pO-cHMFCiXLs1VhD9ii8m9qrV3tWUDpeC2r4s-ByRFxi8VStDFU9OddNyBwD94h9B4iKNWMc8qOjiltLm0o%7EPeYfFmjduEAaBO5IqA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.111, 108.138.64.121, 108.138.64.36, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.111|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3030345861 (2.8G) [binary/octet-stream]\n",
      "Saving to: ‘Echo-A-1B5-Init.pth’\n",
      "\n",
      "Echo-A-1B5-Init.pth 100%[===================>]   2.82G  48.6MB/s    in 59s     \n",
      "\n",
      "2023-07-05 01:13:43 (49.3 MB/s) - ‘Echo-A-1B5-Init.pth’ saved [3030345861/3030345861]\n",
      "\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 2.9G Jun 22 04:41 ../../model/Echo-A-1B5-Init.pth\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "!rm -rf ../../model/Echo-A-1B5-Init.pth\n",
    "!cd ../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
    "!ls -alh ../../model/Echo-A-1B5-Init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 983.42it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_dataset.py ../notebook/trainer-validation/infctx-validation-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Code validation via dryrun\n",
    "\n",
    "The following dryrun, helps check that the existing trainer code changes are valid across 2 * 2 data samples.\n",
    "It does not log the run the W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_128_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=128 -c /home/ubuntu/dev-infctx/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    1536 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_128_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=128 -c /home/ubuntu/dev-infctx/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_128_bf16.so\n",
      "Loading extension module wkv_128_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 932.48it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-08de1ca402bb082a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1f76c32471ce85a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2fdd1d8e3fc93ac5_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-05 01:14:29,623] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-05 01:14:29,624] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.332733154296875 seconds\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.060057640075683594 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002532005310058594 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|   | 4/5308 [00:46<17:14:01, 11.70s/it, v_num=0, train/loss=9.500]`Trainer.fit` stopped: `max_steps=2` reached.\n",
      "Epoch 0:   0%|   | 4/5308 [00:46<17:14:02, 11.70s/it, v_num=0, train/loss=9.500]\n"
     ]
    }
   ],
   "source": [
    "# Validate source code and env is working, by doing a short 2 sample dryrun\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/infctx-validation-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline full context (1024) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 1024. Ensuring all data samples fit within the allocated training size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230705_011558-adyn25wr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/adyn25wr\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /home/ubuntu/dev-infctx/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    12288 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=1024 -c /home/ubuntu/dev-infctx/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 917.79it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-08de1ca402bb082a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1f76c32471ce85a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2fdd1d8e3fc93ac5_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-05 01:16:34,157] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-05 01:16:34,159] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length ({self.ctx_len})\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.331202745437622 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06325173377990723 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007474422454833984 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 5308/5308 [1:13:34<00:00,  1.20it/s, v_num=25wr, train/loss=6.0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:00<00:11,  4.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:10,  4.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:00<00:09,  5.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:00<00:09,  5.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:00<00:09,  5.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:01<00:09,  5.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:01<00:08,  5.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:01<00:08,  5.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:01<00:08,  5.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:01<00:08,  5.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:02<00:07,  5.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:02<00:07,  5.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:02<00:07,  5.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:02<00:07,  5.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:02<00:07,  5.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:02<00:06,  5.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:03<00:06,  5.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:03<00:06,  5.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:03<00:06,  5.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:03<00:06,  5.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:03<00:05,  5.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:03<00:05,  5.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:04<00:05,  5.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:04<00:05,  5.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:04<00:05,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:04<00:05,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:04<00:04,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:05<00:04,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:05<00:04,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:05<00:04,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:05<00:04,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:05<00:03,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:05<00:03,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:06<00:03,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:06<00:03,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:06<00:03,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:06<00:03,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:06<00:02,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:07<00:02,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:07<00:02,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:07<00:02,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:07<00:02,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:07<00:01,  5.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [00:07<00:01,  5.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [00:08<00:01,  5.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [00:08<00:01,  5.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [00:08<00:01,  5.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [00:08<00:01,  5.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [00:08<00:00,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [00:08<00:00,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [00:09<00:00,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [00:09<00:00,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [00:09<00:00,  5.59it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [1:13:52<00:00,  1.20it/s, v_num=25wr, train/loss=6.0\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [1:13:52<00:00,  1.20it/s, v_num=25wr, train/loss=6.0`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [1:13:52<00:00,  1.20it/s, v_num=25wr, train/loss=6.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ██▇▇▆▆▆▆▅▆▅▅▅▅▅▅▄▄▅▄▄▄▄▄▄▁▄▃▅▁▃▃▃▄▄▃▄▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.65017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/adyn25wr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230705_011558-adyn25wr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/infctx-validation-full.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Propagation Through Time (512) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 512. This is a less exegerated version of the 128 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230705_023113-6fszt795\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-512-segmented (train-ctx=512, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/6fszt795\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=512 -c /home/ubuntu/dev-infctx/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    6144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-exp/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=512 -c /home/ubuntu/dev-infctx/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_512_bf16.so\n",
      "Loading extension module wkv_512_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 902.39it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-08de1ca402bb082a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1f76c32471ce85a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2fdd1d8e3fc93ac5_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-05 02:31:49,583] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-05 02:31:49,584] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.329357385635376 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06124377250671387 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006873607635498047 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 5308/5308 [1:28:32<00:00,  1.00s/it, v_num=t795, train/loss=6.2\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:00<00:20,  2.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:19,  2.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:01<00:18,  2.72it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:01<00:18,  2.75it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:01<00:17,  2.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:02<00:17,  2.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:02<00:17,  2.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:02<00:16,  2.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:03<00:16,  2.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:03<00:15,  2.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:03<00:15,  2.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:04<00:15,  2.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:04<00:14,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:05<00:14,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:05<00:13,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:05<00:13,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:06<00:13,  2.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:06<00:12,  2.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:06<00:12,  2.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:07<00:12,  2.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:07<00:11,  2.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:07<00:11,  2.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:08<00:10,  2.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:08<00:10,  2.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:08<00:10,  2.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:09<00:09,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:09<00:09,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:09<00:09,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:10<00:08,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:10<00:08,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:10<00:08,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:11<00:07,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:11<00:07,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:11<00:07,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:12<00:06,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:12<00:06,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:13<00:05,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:13<00:05,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:13<00:05,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:14<00:04,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:14<00:04,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:14<00:04,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:15<00:03,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [00:15<00:03,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [00:15<00:03,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [00:16<00:02,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [00:16<00:02,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [00:16<00:02,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [00:17<00:01,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [00:17<00:01,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [00:17<00:01,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [00:18<00:00,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [00:18<00:00,  2.86it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [1:28:59<00:00,  1.01s/it, v_num=t795, train/loss=6.2\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [1:28:59<00:00,  1.01s/it, v_num=t795, train/loss=6.2`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [1:28:59<00:00,  1.01s/it, v_num=t795, train/loss=6.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇█▆▆▆▆▆▆▅▅▅▅▅▄▅▅▄▄▅▄▄▄▄▄▃▁▄▃▅▁▃▃▃▄▄▃▄▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 6.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.94271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-512-segmented (train-ctx=512, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/6fszt795\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230705_023113-6fszt795/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/infctx-validation-segmented-512.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Propagation Through Time (128) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 128. Forcing all data samples to be segmented 8 times, via \"Truncated Back-Propagation Through Time\"\n",
    "> PS: Weights and biases logging is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230705_040135-arjrziht\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-segmented (train-ctx=128, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/arjrziht\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_128_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 911.01it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-08de1ca402bb082a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1f76c32471ce85a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2fdd1d8e3fc93ac5_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-05 04:01:50,601] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-05 04:01:50,602] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3323559761047363 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06181478500366211 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005953311920166016 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 5308/5308 [8:32:06<00:00,  5.79s/it, v_num=ziht, train/loss=6.7\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:01<01:16,  1.45s/it]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:02<01:14,  1.43s/it]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:04<01:12,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:05<01:10,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:07<01:09,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:08<01:08,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:09<01:06,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:11<01:05,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:12<01:03,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:14<01:02,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:15<01:00,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:17<00:59,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:18<00:58,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:19<00:56,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:21<00:55,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:22<00:53,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:24<00:52,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:25<00:50,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:26<00:49,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:28<00:48,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:29<00:46,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:31<00:45,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:32<00:43,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:33<00:42,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:35<00:41,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:36<00:39,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:38<00:38,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:39<00:36,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:40<00:35,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:42<00:33,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:43<00:32,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:45<00:31,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:46<00:29,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:48<00:28,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:49<00:26,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:50<00:25,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:52<00:24,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:53<00:22,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:55<00:21,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:56<00:19,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:58<00:18,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:59<00:16,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [01:00<00:15,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [01:02<00:14,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [01:03<00:12,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [01:05<00:11,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [01:06<00:09,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [01:07<00:08,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [01:09<00:07,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [01:10<00:05,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [01:12<00:04,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [01:13<00:02,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [01:14<00:01,  1.41s/it]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [8:33:31<00:00,  5.80s/it, v_num=ziht, train/loss=6.7\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [8:33:31<00:00,  5.80s/it, v_num=ziht, train/loss=6.7`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [8:33:31<00:00,  5.80s/it, v_num=ziht, train/loss=6.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ██▇▇▆▆▆▅▄▅▅▅▄▄▅▅▄▄▅▄▄▄▃▃▃▁▃▃▅▁▃▃▂▄▄▃▃▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 6.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 6.53357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-segmented (train-ctx=128, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/arjrziht\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230705_040135-arjrziht/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/infctx-validation-segmented.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last segmented (128) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 128. Only using the last segment. (This replicates previous known regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230705_123606-lk8dbb2g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-last-segment (train-ctx=128, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/lk8dbb2g\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_128_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 913.79it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-08de1ca402bb082a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1f76c32471ce85a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2fdd1d8e3fc93ac5_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-05 12:36:21,992] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-05 12:36:21,993] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.32796049118042 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06155085563659668 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006616115570068359 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/5308 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 5308/5308 [2:57:30<00:00,  2.01s/it, v_num=bb2g, train/loss=7.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:01<01:16,  1.44s/it]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:02<01:13,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:04<01:12,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:05<01:10,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:07<01:08,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:08<01:07,  1.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:09<01:06,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:11<01:04,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:12<01:03,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:14<01:01,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:15<01:00,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:16<00:58,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:18<00:57,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:19<00:56,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:20<00:54,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:22<00:53,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:23<00:51,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:25<00:50,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:26<00:48,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:27<00:47,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:29<00:46,  1.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:30<00:44,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:32<00:43,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:33<00:41,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:34<00:40,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:36<00:38,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:37<00:37,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:38<00:36,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:40<00:34,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:41<00:33,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:43<00:31,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:44<00:30,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:45<00:29,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:47<00:27,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:48<00:26,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:49<00:24,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:51<00:23,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:52<00:22,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:54<00:20,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:55<00:19,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:56<00:18,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:58<00:16,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:59<00:15,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [01:01<00:13,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [01:02<00:12,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [01:03<00:11,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [01:05<00:09,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [01:06<00:08,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [01:07<00:06,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [01:09<00:05,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [01:10<00:04,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [01:12<00:02,  1.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [01:13<00:01,  1.39s/it]\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [2:58:53<00:00,  2.02s/it, v_num=bb2g, train/loss=7.1\u001b[A\n",
      "Epoch 0: 100%|█| 5308/5308 [2:58:53<00:00,  2.02s/it, v_num=bb2g, train/loss=7.1`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5308/5308 [2:58:53<00:00,  2.02s/it, v_num=bb2g, train/loss=7.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ██▆▆▅▆▅▅▄▅▄▄▄▄▄▄▃▄▄▃▃▃▃▃▃▁▃▂▅▂▃▃▂▃▄▂▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 7.15625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 6.99884\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-last-segment (train-ctx=128, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/lk8dbb2g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230705_123606-lk8dbb2g/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/infctx-validation-last-segment.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
