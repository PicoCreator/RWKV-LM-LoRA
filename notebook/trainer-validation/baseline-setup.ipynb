{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfCtx trainer baseline setup\n",
    "The trainer validation is mostly done using the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "Typically with the following dataset\n",
    "- \"teven/enwiki_10k\" dataset, chunked to 1024 token sizes\n",
    "\n",
    "The following notebook, helps perform the basic download and setup for the \"init model\", and \"test dataset\". Which is used as a reference point for all other validation processes (unless stated otherwise)\n",
    "\n",
    "Generally you only need to do this once\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    ">\n",
    "> All training runs (except dryrun) is configured to log to weights and bias, comment out the logger in the config file if you want to avoid this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "!python -m pip install datasets transformers \n",
    "!python -m pip install lightning==2.0.4 deepspeed==0.9.5\n",
    "!python -m pip install ninja numexpr jsonargparse 'jsonargparse[signatures]'\n",
    "!python -m pip install lm-dataformat ftfy sentencepiece tokenizers wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Preparing the init model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-09 05:50:16--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 65.8.243.16, 65.8.243.46, 65.8.243.92, ...\n",
      "Connecting to huggingface.co (huggingface.co)|65.8.243.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1689141016&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTE0MTAxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzBlYzcyMTRlZDE2NzM3YTYzNDgyNTRlNmY5NmQ4Y2RjMDRkM2I1ZWZiZDVmNTNmZTkzMzc2MDdlYTQyYjViOWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=iKAiomdiJ2ozlWxz4oPdqWyGJlQrT3tZ12ygG6zWwpGMfJMeAxomB%7EDh5malirVHqmvjSA-YLV1z0GAogkt91VTcKZCZQoFwYMLSDFFbGccGNKA5n8E6uzvbLe%7ESJNSe4ADiMy3--cyxtSITH2E6WbWFcX1k5nVWAFDAjDUr8wYE0FDH-tu--n-ZpFm1XQwhvVdow7jW0lGa-OJe-4orF2ZWJzE2c1-DGp-q0XvyD2jghe8rjy-FHhOdXHUj2jMK4AqomLUoVETw6skBooWwGuZL5S8Mr3UKKLh1CxqRykI3fyhhkhfM8Rb12XoBO1NADzAC27j0wabcCUppfGOQxg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-09 05:50:17--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1689141016&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTE0MTAxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzBlYzcyMTRlZDE2NzM3YTYzNDgyNTRlNmY5NmQ4Y2RjMDRkM2I1ZWZiZDVmNTNmZTkzMzc2MDdlYTQyYjViOWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=iKAiomdiJ2ozlWxz4oPdqWyGJlQrT3tZ12ygG6zWwpGMfJMeAxomB%7EDh5malirVHqmvjSA-YLV1z0GAogkt91VTcKZCZQoFwYMLSDFFbGccGNKA5n8E6uzvbLe%7ESJNSe4ADiMy3--cyxtSITH2E6WbWFcX1k5nVWAFDAjDUr8wYE0FDH-tu--n-ZpFm1XQwhvVdow7jW0lGa-OJe-4orF2ZWJzE2c1-DGp-q0XvyD2jghe8rjy-FHhOdXHUj2jMK4AqomLUoVETw6skBooWwGuZL5S8Mr3UKKLh1CxqRykI3fyhhkhfM8Rb12XoBO1NADzAC27j0wabcCUppfGOQxg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.160.143.2, 18.160.143.124, 18.160.143.47, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.160.143.2|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3030345861 (2.8G) [binary/octet-stream]\n",
      "Saving to: ‘Echo-A-1B5-Init.pth’\n",
      "\n",
      "Echo-A-1B5-Init.pth 100%[===================>]   2.82G   124MB/s    in 24s     \n",
      "\n",
      "2023-07-09 05:50:41 (122 MB/s) - ‘Echo-A-1B5-Init.pth’ saved [3030345861/3030345861]\n",
      "\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 2.9G Jun 22 04:41 ../../model/Echo-A-1B5-Init.pth\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "!rm -rf ../../model/Echo-A-1B5-Init.pth\n",
    "!cd ../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
    "!ls -alh ../../model/Echo-A-1B5-Init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 63.58it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_dataset.py ../notebook/trainer-validation/config/baseline-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Code validation via dryrun\n",
    "\n",
    "The following dryrun, help do a basic check that the existing trainer code changes are valid across 2 * 2 data samples.\n",
    "\n",
    "If this check fail, its most probably a code / envrionment setup issue (no further checks needed)\n",
    "\n",
    "It does not log the run the W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-27 01:26:35,046] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "Global seed set to 3941088705\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_128_bf16...\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1299.35it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-a5ff2bd3153154c2_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-a154fa0ceb5b4205_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b21a3f1ffa8c1daa_*_of_00016.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-27 01:26:53,082] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3084301948547363 seconds\n",
      "Rank: 0 partition count [1] and sizes[(1112207360, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 906 M \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.1 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,448.829 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/5318 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/picocreator/rwkv-proj/infctx-scratch/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/picocreator/rwkv-proj/infctx-scratch/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 41, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 91, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1018, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 133, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 218, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 185, in run\n",
      "    self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 260, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 140, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1256, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 155, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 256, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 225, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 92, in optimizer_step\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 307, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 287, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 328, in training_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/overrides/base.py\", line 90, in forward\n",
      "    output = self._forward_module.training_step(*inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/infctx-scratch/RWKV-v4neo/src/model.py\", line 1143, in training_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/infctx-scratch/RWKV-v4neo/src/model.py\", line 953, in compute_loss\n",
      "    states = BlockStateList.create(self.n_layer, B, C, seq.device,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/infctx-scratch/RWKV-v4neo/src/model.py\", line 169, in create\n",
      "    result = BlockStateList.empty(N, B, C, device, dtype, att_channels)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/infctx-scratch/RWKV-v4neo/src/model.py\", line 189, in empty\n",
      "    return BlockStateList(att_shift_channel_states, ffn_shift_states, wkv_shift_channel_states)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: BlockStateList.__init__() missing 1 required positional argument: 'att_channels'\n"
     ]
    }
   ],
   "source": [
    "# Validate source code and env is working, by doing a short 2 sample dryrun\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/config/baseline-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Baseline full context (1024) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 1024. Ensuring all data samples fit within the allocated training size. And is used as the baseline loss comparision for several experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 20:53:18,310] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230701_205320-k8flu72z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/k8flu72z\u001b[0m\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 675.41it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3d43d1724bef83d7_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5033407f38c97f24.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-78e7f3a5f1679aa4_*_of_00016.arrow\n",
      "/home/picocreator/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-01 20:53:34,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/picocreator/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/picocreator/rwkv-proj/infctx-dev/checkpoint/trainer-validaiton/infctx-validation-full exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.312431812286377 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 5318/5318 [51:02<00:00,  1.74it/s, v_num=u72z, train/loss=5.940\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:00<00:08,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:07,  6.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:00<00:07,  7.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:00<00:06,  7.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:00<00:06,  7.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:00<00:06,  7.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:00<00:06,  7.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:01<00:06,  7.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:01<00:05,  7.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:01<00:05,  7.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:01<00:05,  7.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:01<00:05,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:01<00:05,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:01<00:05,  7.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:01<00:05,  7.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:02<00:04,  7.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:02<00:04,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:02<00:04,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:02<00:04,  7.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:02<00:04,  7.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:02<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:02<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:02<00:03,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:03<00:03,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:03<00:03,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:03<00:03,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:03<00:03,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:03<00:03,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:03<00:03,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:03<00:03,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:03<00:02,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:04<00:02,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:04<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:05<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:05<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [00:05<00:01,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [00:05<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [00:06<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [00:06<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940\u001b[A\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ██▇▇█▆▇▆▄▅▄▃▄▄▄▄▂▃▃▃▃▃▃▄▁▂▂▃▃▂▂▂▂▃▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.96875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.67332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/k8flu72z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230701_205320-k8flu72z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 new_train.py fit -c ../notebook/trainer-validation/config/baseline-1024.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
