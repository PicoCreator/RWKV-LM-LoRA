{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuseNet 1B4 (Basemodel Training)\n",
    "This model is a custom 1.4B model containing\n",
    "- 96 layers\n",
    "- 1024 embedding size\n",
    "\n",
    "It was initialized using the original RWKV trainer here : https://github.com/PicoCreator/RWKV-LM-LoRA/blob/picocreator-init-memory-experiment/notebook/echo-B-1B4-training.ipynb\n",
    "\n",
    "This is just a silly experiment of using RWKV with music\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-19 17:56:18--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.129, 99.84.108.87, 99.84.108.55, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/aca2f7f217b1d21de5bbf528588684c3f8b2ea16d1b431c551f1681e58ec2de3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-B-1B4-Init.pth%3B+filename%3D%22Echo-B-1B4-Init.pth%22%3B&Expires=1690048578&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDA0ODU3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2L2FjYTJmN2YyMTdiMWQyMWRlNWJiZjUyODU4ODY4NGMzZjhiMmVhMTZkMWI0MzFjNTUxZjE2ODFlNThlYzJkZTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=NL2m85VQAUH2BkbPCqs6gnSwHtrQ9Zq6jJKTiyx3b1SBO-aLtc99mAJGgonSCI04kliHp1ZfTUcGl26hBWuBDs4bsAynFkEo2Of19Q5KncIQk0g4%7EKp5LMCP8Q1lciiGTv7vpa6eVj3dtD3bsdjlCFwU0pR3MHQYQgUpg3-aMRBsWtJwTjRiWUmX9vNvMD5ZDmNDmgR3p-%7ESu%7ETcFd7b2rCoAkD-mR4TzVhcSmRFda1Q2TeC8ArGMOxgk1P0PwYi30J7lQ-5X24ns%7EEitzW51tm9wKoDQkqfkEqVtsjW6eqxAs2SwObrZzGoehZXOvoL03w-FvEw-3t8Ro5P0Rfi5Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-19 17:56:18--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/aca2f7f217b1d21de5bbf528588684c3f8b2ea16d1b431c551f1681e58ec2de3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-B-1B4-Init.pth%3B+filename%3D%22Echo-B-1B4-Init.pth%22%3B&Expires=1690048578&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDA0ODU3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2L2FjYTJmN2YyMTdiMWQyMWRlNWJiZjUyODU4ODY4NGMzZjhiMmVhMTZkMWI0MzFjNTUxZjE2ODFlNThlYzJkZTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=NL2m85VQAUH2BkbPCqs6gnSwHtrQ9Zq6jJKTiyx3b1SBO-aLtc99mAJGgonSCI04kliHp1ZfTUcGl26hBWuBDs4bsAynFkEo2Of19Q5KncIQk0g4%7EKp5LMCP8Q1lciiGTv7vpa6eVj3dtD3bsdjlCFwU0pR3MHQYQgUpg3-aMRBsWtJwTjRiWUmX9vNvMD5ZDmNDmgR3p-%7ESu%7ETcFd7b2rCoAkD-mR4TzVhcSmRFda1Q2TeC8ArGMOxgk1P0PwYi30J7lQ-5X24ns%7EEitzW51tm9wKoDQkqfkEqVtsjW6eqxAs2SwObrZzGoehZXOvoL03w-FvEw-3t8Ro5P0Rfi5Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.67.76.37, 18.67.76.40, 18.67.76.111, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.67.76.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2825882403 (2.6G) [application/zip]\n",
      "Saving to: ‘Echo-B-1B4-Init.pth’\n",
      "\n",
      "Echo-B-1B4-Init.pth 100%[===================>]   2.63G  46.4MB/s    in 64s     \n",
      "\n",
      "2023-07-19 17:57:22 (42.2 MB/s) - ‘Echo-B-1B4-Init.pth’ saved [2825882403/2825882403]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First lets get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "#\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../../model/\n",
    "!mkdir -p ../../../datapath/\n",
    "!mkdir -p ../../../checkpoint/\n",
    "!cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_2_offload\n",
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/breadbrowser-music/notebook/experiment/breadbrowser-music\n",
      "TRAINER_DIR: /home/ubuntu/breadbrowser-music/RWKV-v4neo\n",
      "PROJECT_DIR: /home/ubuntu/breadbrowser-music\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"Musenet-1B4 L96-D1024\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/ubuntu/.cache/huggingface/datasets/breadlicker45___csv/breadlicker45--musenet-encoders-40k-44cc13ced585f16a/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 674.00it/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 264/264 [00:00<00:00, 3.09MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|███| 750k/750k [00:00<00:00, 51.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███| 99.0/99.0 [00:00<00:00, 1.22MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Musenet-1B4.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1053079484\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1053079484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230713_163624-p1orui5k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/p1orui5k\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 66.31it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-197c10b1cc695da5_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3a57548a71476a57_*_of_00064.arrow\n",
      "Saving the dataset (0/1 shards):  15%|▏| 20000/135740 [00:00<00:02, 45099.78 exaSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/1 shards):  29%|▎| 39000/135740 [00:00<00:01, 54565.02 exa[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (0/1 shards):  58%|▌| 79000/135740 [00:01<00:00, 62173.89 exa[rank: 6] Global seed set to 1053079484\n",
      "[rank: 1] Global seed set to 1053079484\n",
      "Saving the dataset (0/1 shards):  64%|▋| 87000/135740 [00:01<00:00, 63673.64 exa[rank: 4] Global seed set to 1053079484\n",
      "[rank: 3] Global seed set to 1053079484\n",
      "[rank: 5] Global seed set to 1053079484\n",
      "[rank: 2] Global seed set to 1053079484\n",
      "[rank: 7] Global seed set to 1053079484\n",
      "Saving the dataset (0/1 shards):  76%|▊| 103000/135740 [00:01<00:00, 64326.85 exUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Saving the dataset (0/1 shards):  82%|▊| 111000/135740 [00:01<00:00, 64561.39 exLoading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 0] Global seed set to 1053079484                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-13 16:36:41,860] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-13 16:36:53,972] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-13 16:37:11,409] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-13 16:37:11,521] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-13 16:37:11,594] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-13 16:37:11,623] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-13 16:37:11,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-13 16:37:11,640] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06677055358886719 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10138964653015137 seconds\n",
      "Time to load fused_adam op: 0.10147595405578613 seconds\n",
      "Time to load fused_adam op: 0.10133147239685059 seconds\n",
      "Time to load fused_adam op: 0.10148763656616211 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1015472412109375 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10184621810913086 seconds\n",
      "Time to load fused_adam op: 0.10184621810913086 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06582045555114746 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10200905799865723 seconds\n",
      "Time to load utils op: 0.10179543495178223 seconds\n",
      "Time to load utils op: 0.10188770294189453 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10148453712463379 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10185384750366211 seconds\n",
      "Time to load utils op: 0.10205650329589844 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10184407234191895 seconds\n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002803802490234375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002830028533935547 seconds\n",
      "Time to load utils op: 0.0002961158752441406 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00030541419982910156 seconds\n",
      "Time to load utils op: 0.0002944469451904297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028228759765625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027179718017578125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00051116943359375 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   6%| | 1000/16968 [36:41<9:45:51,  2.20s/it, v_num=ui5k, train/loss=5./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 16968/16968 [10:26:22<00:00,  2.21s/it, v_num=ui5k, train/loss=\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                  | 1/86 [00:00<00:24,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▍                  | 2/86 [00:00<00:36,  2.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▋                  | 3/86 [00:01<00:29,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 4/86 [00:01<00:27,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 5/86 [00:01<00:25,  3.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 6/86 [00:01<00:24,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▌                 | 7/86 [00:02<00:23,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 8/86 [00:02<00:26,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 9/86 [00:02<00:24,  3.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██                | 10/86 [00:03<00:23,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎               | 11/86 [00:03<00:22,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▌               | 12/86 [00:03<00:22,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▋               | 13/86 [00:03<00:21,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▉               | 14/86 [00:04<00:22,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏              | 15/86 [00:04<00:22,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 16/86 [00:04<00:21,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▌              | 17/86 [00:05<00:20,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▊              | 18/86 [00:05<00:20,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▉              | 19/86 [00:05<00:20,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 20/86 [00:06<00:20,  3.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▍             | 21/86 [00:06<00:20,  3.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 22/86 [00:06<00:19,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▊             | 23/86 [00:07<00:19,  3.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 24/86 [00:07<00:19,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▏            | 25/86 [00:07<00:18,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▍            | 26/86 [00:07<00:18,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 27/86 [00:08<00:17,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▊            | 28/86 [00:08<00:17,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|██████            | 29/86 [00:08<00:16,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 30/86 [00:09<00:17,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 31/86 [00:09<00:16,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 32/86 [00:09<00:16,  3.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 33/86 [00:10<00:16,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████           | 34/86 [00:10<00:15,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 35/86 [00:10<00:15,  3.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 36/86 [00:10<00:15,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 37/86 [00:11<00:15,  3.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▉          | 38/86 [00:11<00:14,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████████▏         | 39/86 [00:12<00:14,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████▎         | 40/86 [00:12<00:14,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▌         | 41/86 [00:12<00:13,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 42/86 [00:13<00:13,  3.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 43/86 [00:13<00:13,  3.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 44/86 [00:13<00:13,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▍        | 45/86 [00:13<00:12,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████▋        | 46/86 [00:14<00:12,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▊        | 47/86 [00:14<00:11,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 48/86 [00:15<00:11,  3.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 49/86 [00:15<00:11,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 50/86 [00:15<00:11,  3.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 51/86 [00:15<00:10,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▉       | 52/86 [00:15<00:10,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 53/86 [00:16<00:10,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 54/86 [00:16<00:09,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 55/86 [00:16<00:09,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 56/86 [00:16<00:09,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▉      | 57/86 [00:17<00:08,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████▏     | 58/86 [00:17<00:08,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 59/86 [00:17<00:08,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▌     | 60/86 [00:17<00:07,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 61/86 [00:18<00:07,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 62/86 [00:18<00:07,  3.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 63/86 [00:18<00:06,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 64/86 [00:18<00:06,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▌    | 65/86 [00:19<00:06,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 66/86 [00:19<00:05,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 67/86 [00:19<00:05,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 68/86 [00:20<00:05,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▍   | 69/86 [00:20<00:05,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 70/86 [00:21<00:04,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▊   | 71/86 [00:21<00:04,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████   | 72/86 [00:21<00:04,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 73/86 [00:22<00:03,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 74/86 [00:22<00:03,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 75/86 [00:22<00:03,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 76/86 [00:22<00:02,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████  | 77/86 [00:23<00:02,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 78/86 [00:23<00:02,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 79/86 [00:23<00:02,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 80/86 [00:23<00:01,  3.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 81/86 [00:23<00:01,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████▏| 82/86 [00:24<00:01,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▎| 83/86 [00:24<00:00,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▌| 84/86 [00:24<00:00,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████████████▊| 85/86 [00:24<00:00,  3.42it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 16968/16968 [10:26:54<00:00,  2.22s/it, v_num=ui5k, train/loss=\u001b[A\n",
      "Epoch 0: 100%|█| 16968/16968 [10:26:54<00:00,  2.22s/it, v_num=ui5k, train/loss=\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 16968/16968 [10:27:08<00:00,  2.22s/it, v_num=ui5k, train/loss=\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▂▁▄▁▁▂▁▁▂▁▁▁▁█▁▂▁▁▃▁▁▂▃▂▁▁▁▁▂▄▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▅▅▅▅▁▅▄▄▄▄▄▄▄▄▄▄▃▄▃▃▄▄▃▄▄▃▄▃▃▂▃▃▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1899\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.54688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.2932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/p1orui5k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230713_163624-p1orui5k/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Musenet-1B4.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Stage1.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 03:17 ../model/Echo-B-1B4-Stage1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Musenet-1B4/last.ckpt\" \"../model/Musenet-1B4.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Musenet-1B4.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
