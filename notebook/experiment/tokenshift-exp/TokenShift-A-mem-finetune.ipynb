{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Token Shift Experiment A (Memory Finetune)\n",
    "This continues off from `./TokenShift-A-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "This is done generally in 3 Tune stages\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set.\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens.\n",
    "- Tune 3: Mid ctx size (1024), stage 2, scaled up to 1024 context sizes.\n",
    "\n",
    "In all cases, the input tokens is always masked. And we intentionally use the limited word set for memory training, which matches the same wordset used in the original memory evaluation of raven pretrained models. This is intentional to serve as both consistent comparision between experiments, and resonable training time.\n",
    "\n",
    "One of the issue faced previously with an excessive large word set, is that the model would be required to see \"new words\" atleast a few time before being able to train the memory process. This drastically slowed down the process as the large word list meant the model was constantly spending time learning new words (instead of memory training).\n",
    "\n",
    "If we want to increase the number / type of words the model can handle for memory training, that can be done later as a stage 4 memory tune if needed. But that exceeds the current requirements for the memory experiment process.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init required dirs\n",
    "# !mkdir -p ../../../model/\n",
    "# !mkdir -p ../../../datapath/\n",
    "# !mkdir -p ../../../checkpoint/\n",
    "\n",
    "# # Download the Stage2.pth file\n",
    "# !rm -rf ../../../model/TokenShift-A-Stage2.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Stage2.pth\n",
    "# !ls -alh ../../../model/TokenShift-A-Stage2.pth\n",
    "\n",
    "# Other models to skip steps if wanted\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Tune1.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Tune2.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Tune3.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Tune4.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp\n",
      "INFERENCE_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5x\n",
      "TRAINER_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v4neo\n",
      "PROJECT_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"(8x3090) TokenShift-A\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5x/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ./dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ./dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ./dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ./dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ./dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ./dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ./dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ./dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ./dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ./dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 21M\n",
      "drwxr-xr-x 2 root root  330 Jul 15 15:00 .\n",
      "drwxr-xr-x 6 root root 4.0K Jul 15 14:59 ..\n",
      "-rw-r--r-- 1 root root 614K Jul 15 15:00 word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Jul 15 15:00 word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 724K Jul 15 15:00 word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 838K Jul 15 15:00 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root 850K Jul 15 15:00 word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Jul 15 15:00 word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 966K Jul 15 15:00 word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.4M Jul 15 15:00 word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 967K Jul 15 15:00 word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.6M Jul 15 15:00 word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 15 15:00 word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 15 15:00 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-5-count.jsonl  5  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-10-count.jsonl 10 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-15-count.jsonl 15 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-20-count.jsonl 20 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-25-count.jsonl 25 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-40-count.jsonl 40 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-50-count.jsonl 50 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-60-count.jsonl 80 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-100-count.jsonl 100 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-248374da0b936b0e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 243.94it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-248374da0b936b0e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 349.32it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/TokenShift-A-mem-finetune-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-A-mem-finetune-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3204158003\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3204158003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230715_150021-ty4q0dit\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) TokenShift-A - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ty4q0dit\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-248374da0b936b0e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 383.01it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-248374da0b936b0e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e572802ff6d91321_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-248374da0b936b0e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7eb1dc89687ef4f7_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3204158003                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-15 15:00:32,842] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 3204158003\n",
      "[rank: 7] Global seed set to 3204158003\n",
      "[rank: 4] Global seed set to 3204158003\n",
      "[rank: 1] Global seed set to 3204158003\n",
      "[rank: 2] Global seed set to 3204158003\n",
      "[rank: 5] Global seed set to 3204158003\n",
      "[rank: 6] Global seed set to 3204158003\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 3] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-15 15:00:45,752] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-15 15:00:47,122] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-15 15:00:48,272] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-15 15:00:48,308] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-15 15:00:48,331] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-15 15:00:48,342] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3204158003\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-15 15:00:48,344] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0629432201385498 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10141539573669434 seconds\n",
      "Time to load fused_adam op: 0.10141992568969727 seconds\n",
      "Time to load fused_adam op: 0.10138535499572754 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10131621360778809 seconds\n",
      "Time to load fused_adam op: 0.10173511505126953 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10178208351135254 seconds\n",
      "Time to load fused_adam op: 0.10178065299987793 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06490588188171387 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10177087783813477 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10184836387634277 seconds\n",
      "Time to load utils op: 0.1020956039428711 seconds\n",
      "Time to load utils op: 0.10180974006652832 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10141897201538086 seconds\n",
      "Time to load utils op: 0.1015629768371582 seconds\n",
      "Time to load utils op: 0.1015312671661377 seconds\n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002601146697998047 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025200843811035156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000247955322265625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002906322479248047 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000213623046875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00034809112548828125 seconds\n",
      "Time to load utils op: 0.0003554821014404297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005235671997070312 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 128 M \n",
      "1 | blocks | ModuleList | 1.0 B \n",
      "2 | ln_out | LayerNorm  | 5.1 K \n",
      "3 | head   | Linear     | 128 M \n",
      "--------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,120.512 Total estimated model params size (MB)\n",
      "Epoch 0:  18%|▏| 800/4371 [04:04<18:13,  3.27it/s, v_num=0dit, train/loss=5.940]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 4371/4371 [22:34<00:00,  3.23it/s, v_num=0dit, train/loss=4.940\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▃▂▁▂▂▁▇▁▁▁▁▄▂▂▂▇█▄▃▁▄▃▁▃▇▃▁▂▂▇▁▂▂▁▂▁▁▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▃▄▇▃▄▃▅▆▂▇▄▁▂▇▃▆▃▁▄▁▁▂▆▆▁▆▃▂▁▁▃▇▃▂▅▁▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 4370\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 34960\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) TokenShift-A - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ty4q0dit\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230715_150021-ty4q0dit/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-mem-finetune-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-A-mem-finetune-1/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 222 params 1280128000 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-A-Tune1.pth\n",
      "-rw-r--r-- 1 root root 4.8G Jul 15 15:24 ../model/TokenShift-A-Tune1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-A-mem-finetune-1/last.ckpt\" \\\n",
    "        \"../model/TokenShift-A-Tune1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-A-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RWKV_HEAD_QK_DIM 0 RWKV_JIT_ON 1\n",
      "\n",
      "blocks.0.att.key.weight                  float32    cuda:0\n",
      "blocks.0.att.output.weight               float32    cuda:0\n",
      "blocks.0.att.receptance.weight           float32    cuda:0\n",
      "blocks.0.att.time_mix_k                  float32    cuda:0\n",
      "blocks.0.att.time_mix_r                  float32    cuda:0\n",
      "blocks.0.att.time_mix_v                  float32    cuda:0\n",
      "blocks.0.att.value.weight                float32    cuda:0\n",
      "blocks.0.ffn.key.weight                  float32    cuda:0\n",
      "blocks.0.ffn.receptance.weight           float32    cuda:0\n",
      "blocks.0.ffn.time_mix_k                  float32    cuda:0\n",
      "blocks.0.ffn.time_mix_r                  float32    cuda:0\n",
      "blocks.0.ffn.value.weight                float32    cuda:0\n",
      "blocks.0.ln0.bias                        float32    cuda:0\n",
      "blocks.0.ln0.weight                      float32    cuda:0\n",
      "blocks.0.ln1.bias                        float32    cuda:0\n",
      "blocks.0.ln1.weight                      float32    cuda:0\n",
      "blocks.0.ln2.bias                        float32    cuda:0\n",
      "blocks.0.ln2.weight                      float32    cuda:0\n",
      "................................................................................................................................................................................\n",
      "emb.weight                               float32    cpu\n",
      "head.weight                              float32    cuda:0\n",
      "ln_out.bias                              float32    cuda:0\n",
      "ln_out.weight                            float32    cuda:0\n",
      "blocks.0.att.time_decay                  float32    cuda:0\n",
      "...........\n",
      "blocks.0.att.time_first                  float32    cuda:0\n",
      "...........###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "Model validation at 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "Model validation at 15 tokens : 66.66666666666666% similarity, with 10 matched token, and 5 token mismatch\n",
      "Model validation at 20 tokens : 55.00000000000001% similarity, with 11 matched token, and 9 token mismatch\n",
      "Model validation at 25 tokens : 40.0% similarity, with 10 matched token, and 15 token mismatch\n",
      "Model validation at 30 tokens : 30.0% similarity, with 9 matched token, and 21 token mismatch\n",
      "Model validation at 35 tokens : 22.857142857142858% similarity, with 8 matched token, and 27 token mismatch\n",
      "Model validation at 40 tokens : 20.0% similarity, with 8 matched token, and 32 token mismatch\n",
      "Model validation at 45 tokens : 17.77777777777778% similarity, with 8 matched token, and 37 token mismatch\n",
      "Model validation at 50 tokens : 18.0% similarity, with 9 matched token, and 41 token mismatch\n",
      "Model validation at 55 tokens : 14.545454545454545% similarity, with 8 matched token, and 47 token mismatch\n",
      "Model validation at 60 tokens : 16.666666666666664% similarity, with 10 matched token, and 50 token mismatch\n",
      "Model validation at 65 tokens : 15.384615384615385% similarity, with 10 matched token, and 55 token mismatch\n",
      "Model validation at 70 tokens : 11.428571428571429% similarity, with 8 matched token, and 62 token mismatch\n",
      "Model validation at 75 tokens : 12.0% similarity, with 9 matched token, and 66 token mismatch\n",
      "Model validation at 80 tokens : 11.25% similarity, with 9 matched token, and 71 token mismatch\n",
      "Model validation at 85 tokens : 11.76470588235294% similarity, with 10 matched token, and 75 token mismatch\n",
      "Model validation at 90 tokens : 12.222222222222221% similarity, with 11 matched token, and 79 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval\n",
    "#\n",
    "# Note that the expected performance \"is not that great\", as the model seems to be only loosely\n",
    "# learning the memorization task, and the instruction propmt. And is seem to be acting more\n",
    "# like an RNG based on the instruct. (Instead of the actual memorization task)\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Tune1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated a single JSONL file with 3558 samples (20 token repeat) - 15 max words - at ./dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ./dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 676 samples (50 token repeat) - 200 max words - at ./dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 1320 samples (50 token repeat) - 100 max words - at ./dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 5000 samples - at ./dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 2633 samples (50 token repeat) - 50 max words - at ./dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 5000 samples - at ./dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 1771 samples (50 token repeat) - 75 max words - at ./dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 3169 samples (30 token repeat) - 25 max words - at ./dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 5222 samples (20 token repeat) - 10 max words - at ./dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 5000 samples - at ./dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 5000 samples - at ./dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 5000 samples - at ./dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 5000 samples - at ./dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 5000 samples - at ./dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 5000 samples - at ./dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 5000 samples - at ./dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 5000 samples - at ./dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 5000 samples - at ./dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 5000 samples - at ./dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 5000 samples - at ./dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 5000 samples - at ./dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 5000 samples - at ./dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 5000 samples - at ./dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 5000 samples - at ./dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 5000 samples - at ./dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ./dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 79M\n",
      "drwxr-xr-x 2 root root  4.0K Jul 15 15:24 .\n",
      "drwxr-xr-x 6 root root  4.0K Jul 15 15:23 ..\n",
      "-rw-r--r-- 1 root root  985K Jul 15 15:24 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jul 15 15:24 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jul 15 15:24 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jul 15 15:24 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jul 15 15:24 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jul 15 15:24 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Jul 15 15:24 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Jul 15 15:24 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  726K Jul 15 15:24 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Jul 15 15:24 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jul 15 15:24 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jul 15 15:24 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Jul 15 15:24 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Jul 15 15:24 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.0M Jul 15 15:24 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Jul 15 15:24 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.5M Jul 15 15:24 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Jul 15 15:24 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Jul 15 15:24 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 1018K Jul 15 15:24 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jul 15 15:24 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  865K Jul 15 15:24 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jul 15 15:24 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jul 15 15:24 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jul 15 15:24 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jul 15 15:24 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Jul 15 15:24 word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  601K Jul 15 15:24 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root   10M Jul 15 15:24 word-200-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl  2  5000 &\n",
    "for i in {5..95..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 5000 & \n",
    "done\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-100-count.jsonl 100 5000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████████████| 29/29 [00:00<00:00, 36461.28it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d816d1b1ca075f1e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 2054.02it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 144.20it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d816d1b1ca075f1e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 123.13it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/TokenShift-A-mem-finetune-2.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-A-mem-finetune-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1171926168\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1171926168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230715_170253-boah6u52\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) TokenShift-A - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/boah6u52\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|█████████████████| 29/29 [00:00<00:00, 104587.12it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d816d1b1ca075f1e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.65it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d816d1b1ca075f1e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a428f3fb8cfcf218_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d816d1b1ca075f1e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c512f3954ad6145e_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 1171926168                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-15 17:03:06,038] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 1171926168\n",
      "[rank: 5] Global seed set to 1171926168\n",
      "[rank: 2] Global seed set to 1171926168\n",
      "[rank: 7] Global seed set to 1171926168\n",
      "[rank: 6] Global seed set to 1171926168\n",
      "[rank: 3] Global seed set to 1171926168\n",
      "[rank: 4] Global seed set to 1171926168\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 3] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-15 17:03:21,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-15 17:03:21,724] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-15 17:03:21,748] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-15 17:03:21,762] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-15 17:03:21,765] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-15 17:03:21,772] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1171926168\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-15 17:03:21,774] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /root/rwkv5x-tokenshift-exp-A/checkpoint/TokenShift-A-mem-finetune-2 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05961108207702637 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10184049606323242 seconds\n",
      "Time to load fused_adam op: 0.1012277603149414 seconds\n",
      "Time to load fused_adam op: 0.10138106346130371 seconds\n",
      "Time to load fused_adam op: 0.10128521919250488 seconds\n",
      "Time to load fused_adam op: 0.1013641357421875 seconds\n",
      "Time to load fused_adam op: 0.10136771202087402 seconds\n",
      "Time to load fused_adam op: 0.10143780708312988 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07016420364379883 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10381054878234863 seconds\n",
      "Time to load utils op: 0.10200834274291992 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10164117813110352 seconds\n",
      "Time to load utils op: 0.10142946243286133 seconds\n",
      "Time to load utils op: 0.10173439979553223 seconds\n",
      "Time to load utils op: 0.1017758846282959 seconds\n",
      "Time to load utils op: 0.10176992416381836 seconds\n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00034880638122558594 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003407001495361328 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00020051002502441406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002529621124267578 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025463104248046875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002551078796386719 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002551078796386719 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004754066467285156 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 128 M \n",
      "1 | blocks | ModuleList | 1.0 B \n",
      "2 | ln_out | LayerNorm  | 5.1 K \n",
      "3 | head   | Linear     | 128 M \n",
      "--------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,120.512 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/16028 [04:04<1:17:35,  3.27it/s, v_num=6u52, train/loss=2.9/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 16028/16028 [1:23:15<00:00,  3.21it/s, v_num=6u52, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/17 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4neo/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▂█▃▅▂▂▂▄▂▃▁▄▁▂▂▂▄▄▂▅▂▅▂▅▄▅▃▃▂▂▁▂▃▄▂▂▂▅▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▂▇▅█▁▃▄▁▁▄▃▃▁▁▁▂▁▁▂▁▁▁▆▇▂▁▂▁▆▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 16027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 128216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) TokenShift-A - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/boah6u52\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230715_170253-boah6u52/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-mem-finetune-2.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-A-mem-finetune-2/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 222 params 1280128000 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-A-Tune2.pth\n",
      "-rw-r--r-- 1 root root 4.8G Jul 15 18:27 ../model/TokenShift-A-Tune2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-A-mem-finetune-2/last.ckpt\" \\\n",
    "        \"../model/TokenShift-A-Tune2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-A-Tune2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a memory eval \n",
    "# #\n",
    "# # While not at its full potential, its memory ability should start emerging\n",
    "# #\n",
    "# !python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Tune2.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 3 : Ramping up the ctx size (1024), memory training\n",
    "\n",
    "- Tune 3: Mid ctx size (1024), same as tune 2, but extended in context size\n",
    "\n",
    "This intentionally a much larger dataset, and lower learning rate to help ensure we push the model to its absolute limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 1000 samples - at ./dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 1000 samples - at ./dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 667 samples (10 token repeat) - 40 max words - at ./dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 1000 samples - at ./dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 1000 samples - at ./dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 587 samples (10 token repeat) - 45 max words - at ./dataset/shuffle-word-45-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 1000 samples - at ./dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 1000 samples - at ./dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 1784 samples (10 token repeat) - 15 max words - at ./dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 557 samples (20 token repeat) - 95 max words - at ./dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 961 samples (20 token repeat) - 55 max words - at ./dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 524 samples (20 token repeat) - 100 max words - at ./dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 1000 samples - at ./dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 1317 samples (10 token repeat) - 20 max words - at ./dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 1000 samples - at ./dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 275 max words - at ./dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 617 samples (20 token repeat) - 85 max words - at ./dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 1062 samples (10 token repeat) - 25 max words - at ./dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 655 samples (20 token repeat) - 80 max words - at ./dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 588 samples (20 token repeat) - 90 max words - at ./dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 873 samples (10 token repeat) - 30 max words - at ./dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 757 samples (10 token repeat) - 35 max words - at ./dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 190 max words - at ./dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 415 max words - at ./dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 873 samples (20 token repeat) - 60 max words - at ./dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 399 samples (20 token repeat) - 110 max words - at ./dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 325 samples (20 token repeat) - 140 max words - at ./dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 362 samples (20 token repeat) - 125 max words - at ./dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 709 samples (20 token repeat) - 75 max words - at ./dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 205 samples (20 token repeat) - 215 max words - at ./dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 409 samples (20 token repeat) - 105 max words - at ./dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 269 samples (20 token repeat) - 195 max words - at ./dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 294 samples (20 token repeat) - 155 max words - at ./dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 250 max words - at ./dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 368 samples (20 token repeat) - 120 max words - at ./dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 255 max words - at ./dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 219 samples (20 token repeat) - 205 max words - at ./dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 747 samples (20 token repeat) - 70 max words - at ./dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 230 max words - at ./dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 220 max words - at ./dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 2627 samples (10 token repeat) - 10 max words - at ./dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 195 samples (20 token repeat) - 240 max words - at ./dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 435 max words - at ./dataset/shuffle-word-435-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 1000 samples - at ./dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 295 max words - at ./dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 315 max words - at ./dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 280 max words - at ./dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 276 samples (20 token repeat) - 185 max words - at ./dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 5568 samples (10 token repeat) - 5 max words - at ./dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 335 max words - at ./dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 811 samples (20 token repeat) - 65 max words - at ./dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 348 samples (20 token repeat) - 130 max words - at ./dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 395 max words - at ./dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 1049 samples (20 token repeat) - 50 max words - at ./dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 260 max words - at ./dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ./dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 285 max words - at ./dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (20 token repeat) - 245 max words - at ./dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 420 max words - at ./dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 330 max words - at ./dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 325 max words - at ./dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 445 max words - at ./dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 375 max words - at ./dataset/shuffle-word-375-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 2000 samples - at ./dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 2000 samples - at ./dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (20 token repeat) - 305 max words - at ./dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 310 max words - at ./dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 294 samples (20 token repeat) - 160 max words - at ./dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 308 samples (20 token repeat) - 150 max words - at ./dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 365 max words - at ./dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 410 max words - at ./dataset/shuffle-word-410-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2000 samples - at ./dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 425 max words - at ./dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 390 max words - at ./dataset/shuffle-word-390-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 2000 samples - at ./dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 300 max words - at ./dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 282 samples (20 token repeat) - 175 max words - at ./dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 137 samples (20 token repeat) - 370 max words - at ./dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 320 max words - at ./dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 450 max words - at ./dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 265 max words - at ./dataset/shuffle-word-265-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 2000 samples - at ./dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 2000 samples - at ./dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 355 max words - at ./dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 213 samples (20 token repeat) - 210 max words - at ./dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 235 max words - at ./dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 318 samples (20 token repeat) - 145 max words - at ./dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 287 samples (20 token repeat) - 165 max words - at ./dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ./dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 350 max words - at ./dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 380 max words - at ./dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 400 max words - at ./dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 430 max words - at ./dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 440 max words - at ./dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 277 samples (20 token repeat) - 200 max words - at ./dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 340 max words - at ./dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 360 max words - at ./dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 290 max words - at ./dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 336 samples (20 token repeat) - 135 max words - at ./dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ./dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 270 max words - at ./dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 385 max words - at ./dataset/shuffle-word-385-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2000 samples - at ./dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 274 samples (20 token repeat) - 180 max words - at ./dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 284 samples (20 token repeat) - 170 max words - at ./dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 382 samples (20 token repeat) - 115 max words - at ./dataset/shuffle-word-115-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2000 samples - at ./dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 2000 samples - at ./dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 2000 samples - at ./dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 2000 samples - at ./dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 2000 samples - at ./dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 2000 samples - at ./dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 2000 samples - at ./dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 2000 samples - at ./dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 2000 samples - at ./dataset/gen-word-115-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 2000 samples - at ./dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 2000 samples - at ./dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 2000 samples - at ./dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 2000 samples - at ./dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 2000 samples - at ./dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 2000 samples - at ./dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 2000 samples - at ./dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 2000 samples - at ./dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 2000 samples - at ./dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 2000 samples - at ./dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 2000 samples - at ./dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 2000 samples - at ./dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2000 samples - at ./dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 2000 samples - at ./dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 2000 samples - at ./dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 2000 samples - at ./dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 2000 samples - at ./dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 2000 samples - at ./dataset/gen-word-195-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 2000 samples - at ./dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 2000 samples - at ./dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 2000 samples - at ./dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 2000 samples - at ./dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 2000 samples - at ./dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 2000 samples - at ./dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 2000 samples - at ./dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 2000 samples - at ./dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 2000 samples - at ./dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 2000 samples - at ./dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 2000 samples - at ./dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 2000 samples - at ./dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 2000 samples - at ./dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 2000 samples - at ./dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 2000 samples - at ./dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 2000 samples - at ./dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 2000 samples - at ./dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 2000 samples - at ./dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 2000 samples - at ./dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 2000 samples - at ./dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 2000 samples - at ./dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 2000 samples - at ./dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 2000 samples - at ./dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 2000 samples - at ./dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 2000 samples - at ./dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 2000 samples - at ./dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 2000 samples - at ./dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 2000 samples - at ./dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 2000 samples - at ./dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 2000 samples - at ./dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 2000 samples - at ./dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 2000 samples - at ./dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 2000 samples - at ./dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 2000 samples - at ./dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 2000 samples - at ./dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 2000 samples - at ./dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 2000 samples - at ./dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 2000 samples - at ./dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 2000 samples - at ./dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 2000 samples - at ./dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 2000 samples - at ./dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 2000 samples - at ./dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 2000 samples - at ./dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 2000 samples - at ./dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 2000 samples - at ./dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 2000 samples - at ./dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 2000 samples - at ./dataset/gen-word-425-count.jsonl\n",
      "## Done ##\n",
      "total 450M\n",
      "drwxr-xr-x 2 root root 8.0K Jul 15 18:43 .\n",
      "drwxr-xr-x 6 root root 4.0K Jul 15 18:41 ..\n",
      "-rw-r--r-- 1 root root 198K Jul 15 18:43 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.1M Jul 15 18:43 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.2M Jul 15 18:43 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 15 18:43 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.4M Jul 15 18:43 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.5M Jul 15 18:43 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.6M Jul 15 18:43 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.7M Jul 15 18:43 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Jul 15 18:43 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.9M Jul 15 18:43 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.0M Jul 15 18:43 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root 245K Jul 15 18:43 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.1M Jul 15 18:43 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.2M Jul 15 18:43 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.3M Jul 15 18:43 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.4M Jul 15 18:43 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.4M Jul 15 18:43 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.5M Jul 15 18:43 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.7M Jul 15 18:43 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Jul 15 18:43 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Jul 15 18:43 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.9M Jul 15 18:43 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root 293K Jul 15 18:43 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.0M Jul 15 18:43 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.1M Jul 15 18:43 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.2M Jul 15 18:43 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.3M Jul 15 18:43 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.4M Jul 15 18:43 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.5M Jul 15 18:43 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.6M Jul 15 18:43 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.7M Jul 15 18:43 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.8M Jul 15 18:43 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.9M Jul 15 18:43 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root 339K Jul 15 18:43 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.0M Jul 15 18:43 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.1M Jul 15 18:43 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Jul 15 18:43 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.3M Jul 15 18:43 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.4M Jul 15 18:43 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Jul 15 18:43 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Jul 15 18:43 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.7M Jul 15 18:43 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.8M Jul 15 18:43 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.8M Jul 15 18:43 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root 391K Jul 15 18:43 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.9M Jul 15 18:43 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1M Jul 15 18:43 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1M Jul 15 18:43 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.2M Jul 15 18:43 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.3M Jul 15 18:43 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.4M Jul 15 18:43 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.5M Jul 15 18:43 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.6M Jul 15 18:43 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.7M Jul 15 18:43 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.8M Jul 15 18:43 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root 435K Jul 15 18:43 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.9M Jul 15 18:43 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.0M Jul 15 18:43 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.1M Jul 15 18:43 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.1M Jul 15 18:43 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.3M Jul 15 18:43 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.4M Jul 15 18:43 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.4M Jul 15 18:43 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.6M Jul 15 18:43 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.6M Jul 15 18:43 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7M Jul 15 18:43 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root 484K Jul 15 18:43 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.8M Jul 15 18:43 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.9M Jul 15 18:43 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.0M Jul 15 18:43 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.1M Jul 15 18:43 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.2M Jul 15 18:43 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.3M Jul 15 18:43 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.4M Jul 15 18:43 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.5M Jul 15 18:43 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.6M Jul 15 18:43 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.7M Jul 15 18:43 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 15 18:43 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.8M Jul 15 18:43 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 147K Jul 15 18:43 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.2M Jul 15 18:43 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.3M Jul 15 18:43 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.4M Jul 15 18:43 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.5M Jul 15 18:43 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.6M Jul 15 18:43 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.7M Jul 15 18:43 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.8M Jul 15 18:43 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.8M Jul 15 18:43 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.9M Jul 15 18:43 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.0M Jul 15 18:43 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 514K Jul 15 18:43 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 567K Jul 15 18:43 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 559K Jul 15 18:43 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root 553K Jul 15 18:43 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 550K Jul 15 18:43 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root 557K Jul 15 18:43 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 547K Jul 15 18:43 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root 548K Jul 15 18:43 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 541K Jul 15 18:43 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root 547K Jul 15 18:43 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 546K Jul 15 18:43 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root 427K Jul 15 18:43 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 543K Jul 15 18:43 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 541K Jul 15 18:43 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root 547K Jul 15 18:43 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 546K Jul 15 18:43 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root 545K Jul 15 18:43 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 542K Jul 15 18:43 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 15 18:43 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 539K Jul 15 18:43 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root 542K Jul 15 18:43 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 536K Jul 15 18:43 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root 380K Jul 15 18:43 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 546K Jul 15 18:43 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 544K Jul 15 18:43 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 542K Jul 15 18:43 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 533K Jul 15 18:43 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 15 18:43 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 540K Jul 15 18:43 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 15 18:43 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 537K Jul 15 18:43 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 15 18:43 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 533K Jul 15 18:43 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root 368K Jul 15 18:43 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 537K Jul 15 18:43 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 15 18:43 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 15 18:43 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 15 18:43 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 15 18:43 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 15 18:43 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 15 18:43 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 536K Jul 15 18:43 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 15 18:43 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root 341K Jul 15 18:43 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 536K Jul 15 18:43 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 533K Jul 15 18:43 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 527K Jul 15 18:43 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 15 18:43 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 527K Jul 15 18:43 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 15 18:43 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 15 18:43 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 15 18:43 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 15 18:43 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root 330K Jul 15 18:43 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 15 18:43 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 15 18:43 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 15 18:43 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 15 18:43 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 15 18:43 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 15 18:43 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root 319K Jul 15 18:43 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 15 18:43 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 15 18:43 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 525K Jul 15 18:43 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 15 18:43 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 15 18:43 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 15 18:43 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 15 18:43 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 15 18:43 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 15 18:43 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 524K Jul 15 18:43 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root 311K Jul 15 18:43 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 527K Jul 15 18:43 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 790K Jul 15 18:43 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 612K Jul 15 18:43 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 609K Jul 15 18:43 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 591K Jul 15 18:43 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 593K Jul 15 18:43 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 586K Jul 15 18:43 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 575K Jul 15 18:43 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 577K Jul 15 18:43 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 581K Jul 15 18:43 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 574K Jul 15 18:43 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 566K Jul 15 18:43 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 119K Jul 15 18:43 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {50..450..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|███████████████| 181/181 [00:00<00:00, 145741.80it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5a26de720865b5a0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 714.65it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 26.73it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5a26de720865b5a0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 77.87it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/TokenShift-A-mem-finetune-3.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-A-mem-finetune-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-mem-finetune-3.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-3 (bs=256, train-ctx=1024, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-A-mem-finetune-3/last.ckpt\" \\\n",
    "        \"../model/TokenShift-A-Tune3.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-A-Tune3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a memory eval \n",
    "# #\n",
    "# # We should start approaching the full potential of the model, unless its able to exceed 250 tokens of memory\n",
    "# #\n",
    "# !python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Tune3.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
