{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Token Shift Experiment A\n",
    "This model is a custom model containing\n",
    "- 12 layers\n",
    "- 2560 embedding size\n",
    "\n",
    "See `./notes.md` for how the init model was initilaized.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-15 01:12:48--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/L12-D2560-init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 13.224.249.10, 13.224.249.43, 13.224.249.44, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.224.249.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/490b49cdae99030f402fa01a60817bb53c67b6164aa3858742ea6b1560b2c4ed?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27L12-D2560-init.pth%3B+filename%3D%22L12-D2560-init.pth%22%3B&Expires=1689613968&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTYxMzk2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzQ5MGI0OWNkYWU5OTAzMGY0MDJmYTAxYTYwODE3YmI1M2M2N2I2MTY0YWEzODU4NzQyZWE2YjE1NjBiMmM0ZWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Zmghrl%7EqBOcsLy6Yb%7ESOJWV5Cy85oQ5-psOE39OoJ1%7Ef6Z2GrQanRuAywj7l4fr9-OJiBjZxr%7E%7EEE9VDANl%7EOtmYoqZKn6hmdimCNi6HFO6Gh0Nr5IA2dMbKv61rfScjK1RDSEy6g-HA7aGfINGwGQqEuAEiLKQYEfUEDLt9nIgFKXBbxYi9ZUcrf6N0lUov%7E-qixQxD0qGrzZX4cmCRs6rIB0P5oRWT-Kejo2qAckVr%7EK3YaB5m3n7z6ZwaXX6JEZik-0vGfmQiipPZPAtqT9NOT-RpjOFi0VOGn5xf3uZkSLGcL2rGGxbHldkdndK3lyIpB-PKo0ZrPWs4l5dnmQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-15 01:12:48--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/490b49cdae99030f402fa01a60817bb53c67b6164aa3858742ea6b1560b2c4ed?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27L12-D2560-init.pth%3B+filename%3D%22L12-D2560-init.pth%22%3B&Expires=1689613968&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTYxMzk2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzQ5MGI0OWNkYWU5OTAzMGY0MDJmYTAxYTYwODE3YmI1M2M2N2I2MTY0YWEzODU4NzQyZWE2YjE1NjBiMmM0ZWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Zmghrl%7EqBOcsLy6Yb%7ESOJWV5Cy85oQ5-psOE39OoJ1%7Ef6Z2GrQanRuAywj7l4fr9-OJiBjZxr%7E%7EEE9VDANl%7EOtmYoqZKn6hmdimCNi6HFO6Gh0Nr5IA2dMbKv61rfScjK1RDSEy6g-HA7aGfINGwGQqEuAEiLKQYEfUEDLt9nIgFKXBbxYi9ZUcrf6N0lUov%7E-qixQxD0qGrzZX4cmCRs6rIB0P5oRWT-Kejo2qAckVr%7EK3YaB5m3n7z6ZwaXX6JEZik-0vGfmQiipPZPAtqT9NOT-RpjOFi0VOGn5xf3uZkSLGcL2rGGxbHldkdndK3lyIpB-PKo0ZrPWs4l5dnmQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.94, 18.155.68.128, 18.155.68.73, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.94|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2560323205 (2.4G) [binary/octet-stream]\n",
      "Saving to: ‘L12-D2560-init.pth’\n",
      "\n",
      "L12-D2560-init.pth   15%[==>                 ] 376.29M  4.75MB/s    eta 4m 3s  "
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../../model/\n",
    "!mkdir -p ../../../datapath/\n",
    "!mkdir -p ../../../checkpoint/\n",
    "!rm -rf ../../../model/L12-D2560-init.pth\n",
    "!cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/L12-D2560-init.pth\n",
    "!ls -alh ../../../model/L12-D2560-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"TokenShift-Exp-A\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/TokenShift-A-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/TokenShift-A-enwiki/last.ckpt\" \"../model/TokenShift-A-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/TokenShift-A-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "# !cd \"{TRAINER_DIR}\" && python3 dragon_test.py ../model/TokenShift-A-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/TokenShift-A-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/TokenShift-A-instruct/last.ckpt\" \"../model/TokenShift-A-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/TokenShift-A-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "# !cd \"{TRAINER_DIR}\" && python3 dragon_test.py \"../model/TokenShift-A-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
